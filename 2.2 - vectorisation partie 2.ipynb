{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Master Informatique, parcours Data Mining\n",
    "\n",
    "### Carnets de note Python pour le cours de Text Mining\n",
    "\n",
    "Julien Velcin, laboratoire ERIC, Université Lyon 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des documents (partie 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Différentes solutions existent pour représenter un document dans un espace vectoriel :\n",
    "\n",
    "* espace des mots (avec différents types de pondération : TF, TFxIDF, OKAPI)\n",
    "* espace sémantique de faible dimension :\n",
    "  * **approche de plongement (*sentence/document embedding*)**\n",
    "  * approche thématique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va déployer deux méthodes simples basées sur des plongements qui ne prennent pas en compte l'ordre des mots :\n",
    "- solution naïve : centre d'inertie des vecteurs codant le sens des mots pris indépendamment\n",
    "- méthode Doc2Vec\n",
    "\n",
    "D'autres méthodes ont été proposés récemment mais elles se basent souvent sur des approches plus complexes voire coûteuses (comme le Transformer). Des éléments sont données à la fin de ce carnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de voir comment construire ces représentations, voyons comment utiliser des plongements de mots pré-appris avec la librairie *spacy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Il faut avoir installé la ressource en local en version \"large\" (d'où le \"lg\" à la fin) si on veut avoir accès aux vecteurs : \n",
    "#python -m spacy download fr_core_news_lg\n",
    "\n",
    "nlp = spacy.load('fr_core_news_lg')\n",
    "#nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première action que nous pouvons faire est d'interroger la librairie sur les mots les plus proches d'une requête :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roi',\n",
       " 'Roi',\n",
       " 'prince',\n",
       " 'monarque',\n",
       " 'empereur',\n",
       " 'régent',\n",
       " 'Empereur',\n",
       " 'suzerain',\n",
       " 'coempereur',\n",
       " 'l\\x92empereur']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_close_words(word):\n",
    "    sim_words = nlp.vocab.vectors.most_similar(\n",
    "        np.asarray([nlp.vocab.vectors[nlp.vocab.strings[word]]]), n=10)\n",
    "    return [nlp.vocab.strings[w] for w in sim_words[0][0]]\n",
    "\n",
    "find_close_words(\"roi\")\n",
    "\n",
    "# attention, nlp.vocab.strings fonctionne dans les deux sens :\n",
    "#  - retourne l'identifiant (unique) correspondant à un mot\n",
    "#  - retourne le mot correspondant à un identifiant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également calculer des similarités entre mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entre roi et reine : 0.628108097894871\n",
      "entre roi et trône : 0.6489303722402299\n",
      "entre roi et oiseau : 0.10113929594015392\n"
     ]
    }
   ],
   "source": [
    "print(\"entre roi et reine : {}\".format(nlp(\"roi\").similarity(nlp(\"reine\"))))\n",
    "print(\"entre roi et trône : {}\".format(nlp(\"roi\").similarity(nlp(\"trône\"))))\n",
    "print(\"entre roi et oiseau : {}\".format(nlp(\"roi\").similarity(nlp(\"oiseau\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs qui représentent les mots sont directement accessibles si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.6351e+00, -4.7504e+00,  2.9866e+00, -6.5048e-02,  5.6314e+00,\n",
       "       -2.6488e+00,  1.6728e+00,  3.6799e+00,  2.2033e+00, -2.3401e-01,\n",
       "        1.7170e+00, -3.0931e+00,  3.8267e+00,  1.8726e-01, -2.2761e+00,\n",
       "       -2.1582e+00,  3.8331e+00, -1.1703e-01,  1.2575e+00,  3.2968e+00,\n",
       "       -1.1991e+00,  5.0724e-01, -2.6204e+00,  2.1288e+00,  9.9307e-01,\n",
       "       -3.1832e+00, -2.3930e+00,  1.3869e+00, -1.7312e+00,  4.5163e+00,\n",
       "        9.9608e-01, -3.1276e+00, -1.5539e+00,  1.0525e+00, -5.3267e-01,\n",
       "       -5.5565e+00,  8.4429e-01,  4.9819e+00, -4.6006e-01,  2.2781e+00,\n",
       "       -1.0639e+00,  1.2235e+00, -4.5606e+00,  3.4419e-01,  1.9167e+00,\n",
       "       -1.5078e+00,  3.6443e+00,  2.8392e+00,  4.9837e-01, -5.5096e-02,\n",
       "       -7.9940e+00, -1.9028e+00, -3.9536e+00,  3.7232e-03,  2.3186e+00,\n",
       "        5.6297e+00,  2.2466e+00,  5.2483e-01,  1.2710e+00, -5.8875e-01,\n",
       "       -2.2971e+00,  1.4909e-01,  2.7780e-03,  3.9281e+00,  6.5166e-01,\n",
       "        8.9464e-02,  3.2467e+00,  9.5456e-01, -2.4231e+00, -4.3828e+00,\n",
       "        1.2746e+00,  1.8955e+00, -2.9509e+00, -5.8867e+00,  3.0693e+00,\n",
       "       -8.0891e-01,  6.9400e-01,  3.8128e+00,  3.5806e+00,  4.2890e+00,\n",
       "       -2.7194e+00,  2.5940e+00,  4.6594e+00,  6.6699e-02, -4.7390e+00,\n",
       "       -1.4998e-01, -9.3746e-01,  4.6751e+00, -1.4972e+00, -1.0867e+01,\n",
       "        4.9722e+00, -3.9660e-01,  9.2787e-01,  4.9520e-01,  2.4602e+00,\n",
       "       -1.0434e+00,  3.0316e+00, -1.7612e-02,  4.9520e+00, -4.8073e+00,\n",
       "        3.8922e-01,  1.0257e+00,  1.0831e-01,  4.0534e+00, -2.1833e+00,\n",
       "        1.5790e+00, -3.2116e+00,  2.6927e+00, -3.8434e-01, -4.7468e+00,\n",
       "        7.2697e-01, -2.1205e+00,  5.9403e-01,  4.7841e+00, -3.8717e+00,\n",
       "       -3.0401e+00, -1.2210e+00, -2.8635e+00, -6.9869e-01,  3.2319e+00,\n",
       "        5.7720e+00,  5.9429e+00, -4.3025e+00, -4.9565e-01, -2.3339e+00,\n",
       "       -1.5692e+00, -6.8929e-01, -1.1271e+00, -1.3472e+00,  2.0474e+00,\n",
       "        4.7208e+00, -4.9401e-01,  1.2751e+00, -3.2224e+00, -1.0313e+00,\n",
       "       -1.7439e+00,  8.7939e-01,  1.2688e-01, -4.7297e+00, -1.5638e+00,\n",
       "       -2.3903e+00,  9.2193e-01,  1.1045e+00, -2.1525e+00,  9.6714e-01,\n",
       "        1.3308e+00, -5.6572e+00, -2.5758e+00,  3.4210e+00,  1.8027e+00,\n",
       "        6.2324e+00,  5.7700e+00,  2.3663e+00,  1.0878e+00, -1.3365e+00,\n",
       "        1.1781e+00,  2.7440e+00,  2.3066e+00,  8.4879e-01,  2.2804e+00,\n",
       "        1.5781e+00, -2.4117e+00, -1.3357e+00,  5.1102e+00, -8.7586e-01,\n",
       "       -2.9436e+00, -1.9356e+00,  7.7401e+00,  5.9926e+00,  2.3883e+00,\n",
       "       -3.1656e+00,  6.4872e+00, -2.2768e+00,  1.4991e+00, -3.8998e+00,\n",
       "        5.2592e-01, -3.8706e-01, -9.6823e-01,  1.4808e+00, -3.9378e+00,\n",
       "       -2.0357e+00, -4.4986e+00,  8.9750e+00,  2.2045e+00,  1.8716e-03,\n",
       "        3.2258e+00,  4.7617e+00, -1.8651e+00, -6.1514e+00,  2.5549e+00,\n",
       "       -1.6786e+00,  1.9050e+00,  3.4051e+00, -4.6565e+00,  4.1841e+00,\n",
       "       -7.8164e+00,  1.6226e+00, -2.7039e+00, -5.7035e-01, -1.3518e+00,\n",
       "       -8.5385e-01, -1.2273e+00,  1.2349e+00, -7.8515e-01,  5.0479e-01,\n",
       "        2.8719e+00,  8.9905e-01,  4.9693e+00, -2.0078e+00,  4.3517e+00,\n",
       "       -1.6480e+00, -5.0611e+00,  6.5107e+00, -3.6784e+00,  3.1451e+00,\n",
       "       -2.8814e+00, -3.1464e+00, -1.5150e-01,  3.1440e+00, -3.6309e+00,\n",
       "       -2.3865e+00,  4.2842e+00,  2.9188e+00, -2.8959e+00,  8.5420e-01,\n",
       "        6.1502e+00,  3.0970e+00, -6.2933e-02, -7.2427e-01, -4.6472e+00,\n",
       "       -5.1277e-01,  3.6802e+00,  4.1581e+00,  1.6980e+00,  3.4822e+00,\n",
       "        1.8467e+00, -2.8244e+00, -8.6421e+00,  1.9419e+00, -2.4484e-01,\n",
       "        2.5395e+00, -2.6593e+00, -2.9772e+00, -1.7609e+00, -1.1823e+00,\n",
       "        6.7310e+00,  2.8250e+00, -2.4596e-01, -4.1260e+00,  4.5138e+00,\n",
       "       -5.3808e+00, -2.3224e+00,  1.9299e+00,  1.6637e+00,  5.3128e+00,\n",
       "       -2.1047e+00, -1.5359e+00, -2.2614e-01, -8.1183e+00,  3.4033e-01,\n",
       "        1.6500e+00, -2.3227e+00,  4.0308e+00,  3.5514e+00,  6.6944e-01,\n",
       "        1.4120e+00, -8.0441e-01, -6.5361e-01, -5.6505e+00,  2.4682e+00,\n",
       "       -9.1946e+00, -6.3852e+00,  8.2396e+00, -2.7895e+00, -2.3564e+00,\n",
       "        8.4714e+00, -2.6244e-01,  4.2987e+00,  1.5561e+00,  2.4657e+00,\n",
       "       -6.1948e+00, -6.6181e-01, -9.9525e-01,  4.2478e+00, -6.1932e+00,\n",
       "       -1.9624e+00, -7.0235e+00,  3.3948e-01, -7.3027e-01, -4.2027e+00,\n",
       "        1.5683e+00, -6.5984e+00, -2.5014e+00,  1.3260e+00, -3.1824e+00,\n",
       "       -5.0466e+00, -5.4672e+00, -4.6966e+00, -5.3983e+00,  1.5300e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"roi\").vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut même résoudre des problèmes d'analogie. Par ex., qu'est-ce qui est à \"femme\" ce que \"homme\" est à \"roi\" ? Ou la relation \"capitale de\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_words_from_vector(vec):\n",
    "    ms = nlp.vocab.vectors.most_similar(np.array([vec]), n=10)\n",
    "    return [nlp.vocab.strings[w] for w in ms[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roi',\n",
       " 'reine',\n",
       " 'Roi',\n",
       " 'prince',\n",
       " 'régent',\n",
       " 'duc',\n",
       " 'princesse',\n",
       " 'monarque',\n",
       " 'suzerain',\n",
       " 'coempereur']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogie = nlp(\"roi\").vector-nlp(\"homme\").vector+nlp(\"femme\").vector\n",
    "close_words_from_vector(analogie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Allemagne',\n",
       " 'l´Allemagne',\n",
       " 'ex-Allemagne',\n",
       " 'lAllemagne',\n",
       " 'Allemagne-',\n",
       " 'Europe',\n",
       " 'Grande-Bretagne',\n",
       " 'ouest-allemande',\n",
       " 'est-allemande',\n",
       " 'Allemagnes']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogie = nlp(\"France\").vector-nlp(\"Paris\").vector+nlp(\"Berlin\").vector\n",
    "close_words_from_vector(analogie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approche naïve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons calculer des représentations vectorielles de documents comme le centre d'inertie des mots qui le composent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(\"datasets\", \"Frank Herbert - Dune.txt\")) as f:\n",
    "    lines = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvelcin/arm/envs/cours21_spacy/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(stop_words=\"english\", max_df=0.5, min_df=3, max_features=1000)\n",
    "tf_vectorizer.fit(lines)\n",
    "D = tf_vectorizer.transform(lines)\n",
    "\n",
    "features = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère la taille des plongements car elle constitue la dimension de l'espace dans lequel on va plonger les documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(nlp.vocab.vectors[nlp.vocab.strings[\"dune\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8608\n"
     ]
    }
   ],
   "source": [
    "ndocs, nwords = D.shape\n",
    "print(ndocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit une fonction qui calcule le centre d'inertie d'un ensemble de vecteurs mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def centre(d):\n",
    "    m = np.zeros(shape=(1,dim))\n",
    "    nbw = 0\n",
    "    for w in d:\n",
    "        try:\n",
    "            v = nlp.vocab.vectors[nlp.vocab.strings[str(w)]]\n",
    "            m = np.append(m, v.reshape((1,dim)), axis=0)     \n",
    "            nbw += 1\n",
    "        except:\n",
    "            pass\n",
    "    seuil = True\n",
    "    if nbw>0:\n",
    "        return (nbw, np.sum(m, axis=0)/nbw) # la normalisation est inutile si on utilise le cosinus\n",
    "    else:\n",
    "        return (0, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis on calcule la représentation pour chaque document du corpus. On en profite pour sauvegarder une liste avec la taille des documents (ici, le nombre de mots ayant un vecteur associé dans le plongement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbw_docs = []\n",
    "i = 0\n",
    "doc_vec = np.zeros(shape=(ndocs,dim))\n",
    "id_docs_nonvides = []\n",
    "for d in tf_vectorizer.inverse_transform(D):\n",
    "    nbw, r = centre(d)\n",
    "    doc_vec[i] = r.reshape((1,dim))\n",
    "    nbw_docs.append(nbw)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.09649992, -3.93840003, -1.81669998, ..., -0.72492999,\n",
       "         2.34240007,  0.1106    ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(doc_vec[0])\n",
    "doc_vec[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 1, 0, 6, 8, 11, 22, 8, 6, 4]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbw_docs[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il ne reste plus qu'à sauvegarder l'information pour une utilisation future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_p = np.array(nbw_docs).reshape(ndocs,1)\n",
    "col_ids = np.arange(1, ndocs+1).reshape(ndocs,1)\n",
    "data_to_save = np.hstack([doc_vec, col_p, col_ids])\n",
    "np.savetxt('vec_doc_naive.csv', data_to_save, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dov2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec est une extension des approches Word2Vec dans lesquelles on ajoute un \"token\" associé à chaque document (ici, un paragraphe). Il existe deux versions de cet algorithme (Le and Mikolov, 2014) :\n",
    "\n",
    "* PV-DM : Distributed Memory Models of Paragraph Vectors\n",
    "* PV-DBOW : Distributed Bag of Words version of Paragraph Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border:0;\">\n",
    "<tr style=\"border:0;\">\n",
    "    <td><img src=\"img/PVDM.png\" style='height: 200px'/></td>\n",
    "    <td><img src=\"img/PVDOBW.png\" style='height: 200px'/></td>\n",
    "    </tr>\n",
    "    </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut formatter les données pour pouvoir les données en entrée de l'algorithme Doc2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# on rajoute une taille minimale dès à présent\n",
    "min_docs = 4\n",
    "\n",
    "tagged_docs = []\n",
    "nbw_docs = []\n",
    "for i, list_tokens in enumerate(tf_vectorizer.inverse_transform(D)):\n",
    "    nbw = len(list_tokens)\n",
    "    nbw_docs.append(nbw)\n",
    "    if nbw > min_docs:        \n",
    "        tagged_docs.append(TaggedDocument(words=list_tokens, tags=[str(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=array(['arrakis', 'begin', 'beginning', 'bene', 'born', 'caladan', 'care',\n",
       "        'dib', 'dune', 'emperor', 'fact', 'gesserit', 'known', 'knows',\n",
       "        'life', 'lived', 'muad', 'padishah', 'place', 'planet', 'special',\n",
       "        'study', 'taking', 'time', 'year', 'years'], dtype='<U13'), tags=['11']),\n",
       " TaggedDocument(words=array(['arrakis', 'boy', 'came', 'mother', 'old', 'paul', 'reached'],\n",
       "       dtype='<U13'), tags=['14']),\n",
       " TaggedDocument(words=array(['ancient', 'atreides', 'caladan', 'change', 'family', 'feeling',\n",
       "        'home', 'night', 'stone', 'weather'], dtype='<U13'), tags=['15']),\n",
       " TaggedDocument(words=array(['allowed', 'bed', 'door', 'lay', 'let', 'moment', 'old', 'passage',\n",
       "        'paul', 'room', 'woman'], dtype='<U13'), tags=['16'])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_docs)\n",
    "tagged_docs[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_d2v = 10\n",
    "\n",
    "#model_doc2vec = Doc2Vec(tagged_docs, vector_size=dim_d2v, window = 3, iter = 1000)\n",
    "model_doc2vec = Doc2Vec(tagged_docs, vector_size=dim_d2v, window = 3)\n",
    "model_doc2vec.train(tagged_docs, total_examples = len(tagged_docs), epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10', 'soo'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1 = set(features)\n",
    "set2 = set(model_doc2vec.wv.index_to_key)\n",
    "set1.difference(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x29914e1c0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_doc2vec.dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8355', 0.9608270525932312), ('7379', 0.9539141654968262), ('7145', 0.9419770836830139), ('1817', 0.9397681355476379), ('8490', 0.9245098829269409), ('7593', 0.9177431464195251), ('7577', 0.9100067019462585), ('6730', 0.9078817367553711), ('7412', 0.9061649441719055), ('6836', 0.8994591236114502)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "test_doc = word_tokenize(\"Dune, the spice planet\".lower())\n",
    "test_doc_vector = model_doc2vec.infer_vector(test_doc)\n",
    "res = model_doc2vec.dv.most_similar(positive = [test_doc_vector])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8355 (0.9608270525932312): DUNE MEN: idiomatic for open sand workers, spice hunters and the like on Arrakis. Sandworkers. Spiceworkers.\n",
      "7379 (0.9539141654968262): Paul looked at her. \"For the Guild's permission to land. The Guild will strand on Arrakis any force that lands without permission.\"\n",
      "7145 (0.9419770836830139): \"The evidence is not here,\" Paul said. \"It's in Tabr sietch, far to the south, but if --\"\n",
      "1817 (0.9397681355476379): \"Not from the deep desert,\" Kynes said. \"Men have walked out of the second zone several times. They've survived by crossing the rock areas where worms seldom go.\"\n",
      "8490 (0.9245098829269409): POLING THE SAND: the art of placing plastic and fiber poles in the open desert wastes of Arrakis and reading the patterns etched on the poles by sandstorms as a clue to weather prediction.\n",
      "7593 (0.9177431464195251): \"His people scream his name as they leap into battle. The women throw their babies at us and hurl themselves onto our knives to open a wedge for their men to attack us. They have no . . . no . . . decency!\"\n",
      "7577 (0.9100067019462585): \"The storm comes, Majesty. I sent them to inspect our perimeter lest the Fremen attack under cover of the sand.\"\n",
      "6730 (0.9078817367553711): \"So it seemed,\" Paul said. \"But this is deep into the desert, for smugglers.\"\n",
      "7412 (0.9061649441719055): \"Is there anything new to see from here?\" Gurney asked. \"We should be getting under cover. The storm is coming.\"\n",
      "6836 (0.8994591236114502): Gurney nodded toward the desert below them. Fremen were going about their business all over the landscape. It struck him that none of them appeared worried by the approach of the worm.\n"
     ]
    }
   ],
   "source": [
    "for i, s in res:\n",
    "    ind_doc = int(i)\n",
    "    print(\"%s (%s): %s\" % (i, s, lines[ind_doc-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(model_doc2vec.dv))\n",
    "type(model_doc2vec.dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5007\n"
     ]
    }
   ],
   "source": [
    "#set_tags = list(model_doc2vec.docvecs.doctags)\n",
    "set_tags = list([t.tags[0] for t in tagged_docs])\n",
    "nb_docs_small = len(set_tags)\n",
    "print(nb_docs_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère le tableau des plongements pour le sauvegarder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5007, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec_doc2vec = np.zeros(shape=(nb_docs_small, dim_d2v))\n",
    "\n",
    "i = 0\n",
    "for t in set_tags:    \n",
    "    doc_vec_doc2vec[i] = model_doc2vec.dv[t]\n",
    "    i += 1\n",
    "\n",
    "doc_vec_doc2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_small = [int(t) for t in set_tags]\n",
    "nbw_docs_small = [nbw_docs[i-1] for i in doc_ids_small]\n",
    "\n",
    "col_p = np.array(nbw_docs_small).reshape(nb_docs_small,1)\n",
    "col_ids = np.array(doc_ids_small).reshape(nb_docs_small,1)\n",
    "data_to_save = np.hstack([doc_vec_doc2vec, col_p, col_ids])\n",
    "np.savetxt('vec_doc_doc2vec.csv', data_to_save, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nombreuses autres méthodes existent pour construire des représentations de documents, par exemple :\n",
    "\n",
    "* InferSent (EMNLP 2017)\n",
    "* Universal Sentence Encoder (EMNLP 2018)\n",
    "* SentenceBERT (EMNLP 2019)\n",
    "\n",
    "N'hésitez pas à consulter la page suivante qui décrit ses approches et comment les implémenter :\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering de documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif est de vous montrer comment utiliser un algorithme simple de clustering (ici, k-means). Bien sûr, l'intérêt d'utiliser un espace vectoriel est de pouvoir utiliser de nombreux autres algorithmes, comme des modèles de mélange, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8608, 300)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 10\n",
    "\n",
    "km_10 = KMeans(n_clusters=k, random_state=0).fit(doc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    2463\n",
       "4    2152\n",
       "7     918\n",
       "3     892\n",
       "5     769\n",
       "6     398\n",
       "1     364\n",
       "2     288\n",
       "9     276\n",
       "0      88\n",
       "dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.Series(km_10.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8608"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_p = np.array(nbw_docs).reshape(ndocs,1)\n",
    "col_ids = np.arange(1, ndocs+1).reshape(ndocs,1)\n",
    "clu_lab = np.array(km_10.labels_).reshape(ndocs,1)\n",
    "data_to_save = np.hstack([doc_vec, col_p, col_ids, clu_lab])\n",
    "np.savetxt('vec_doc_naive_cl10.csv', data_to_save, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem avec la représentation obtenue à l'aide de doc2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_10_doc2vec = KMeans(n_clusters=10, random_state=0).fit(doc_vec_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids_small = [int(t) for t in set_tags]\n",
    "nbw_docs_small = [nbw_docs[i-1] for i in doc_ids_small]\n",
    "clu_lab_small = np.array(km_10_doc2vec.labels_).reshape(nb_docs_small,1)\n",
    "\n",
    "col_p = np.array(nbw_docs_small).reshape(nb_docs_small,1)\n",
    "col_ids = np.array(doc_ids_small).reshape(nb_docs_small,1)\n",
    "data_to_save = np.hstack([doc_vec_doc2vec, col_p, col_ids, clu_lab_small])\n",
    "np.savetxt('vec_doc_doc2vec_clu10.csv', data_to_save, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Références :\n",
    "   \n",
    "* Le, Quoc, and Tomas Mikolov. Distributed representations of sentences and documents. International Conference on Machine Learning (ICML), 2014.\n",
    "* Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data, EMNLP 2017.\n",
    "* Daniel Cera, Yinfei Yanga, Sheng-yi Konga, Nan Huaa, Nicole Limtiacob, Rhomni St. Johna, Noah Constanta, Mario Guajardo-Ce ́spedesa, Steve Yuanc, Chris Tara, Yun-Hsuan Sunga, Brian Stropea. Universal Sentence Encode for Englishr, EMNLP 2018.\n",
    "* Nils Reimers, Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks, EMNLP 2019.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous n'aurons pas le temps de le faire en cours, mais on comprend bien qu'il est également immédiat de déployer des algorithmes de classification à partir de ces représentations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
