{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Master Informatique, parcours Data Mining\n",
    "\n",
    "### Carnets de note Python pour le cours de Text Mining\n",
    "\n",
    "Julien Velcin, laboratoire ERIC, Université Lyon 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prétraitements (partie 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quelques prétraitements à connaître\n",
    "\n",
    "- expressions régulières et nettoyages simples\n",
    "- segmentation en mots (*tokenization*)\n",
    "- mots-outils\n",
    "- stemming et lemmatisation\n",
    "- n-grammes et collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques prétraitements à connaître\n",
    "\n",
    "- **expressions réguières et nettoyages simples**\n",
    "- segmentation en mots (*tokenization*)\n",
    "- mots-outils\n",
    "- stemming et lemmatisation\n",
    "- n-grammes et collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expressions régulières "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "texte = \"\" # chaîne vide\n",
    "with open(\"Frank Herbert - Children of Dune.txt\", \"r\", encoding='utf8') as f:\n",
    "    texte = texte.join(line.rstrip(\"\\n\") + \" \" for line in f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10566\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(\"the\")\n",
    "\n",
    "# cherche toutes les occurrences\n",
    "res = pattern.finditer(texte)\n",
    "\n",
    "start_pattern = [m.start() for m in res]\n",
    "\n",
    "#print(start_pattern)\n",
    "print(len(start_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(720, 725), (9600, 9605), (9631, 9636), (10319, 10324), (15334, 15339), (19340, 19345), (19713, 19718), (24076, 24081), (24412, 24417), (25627, 25632), (26968, 26973), (28993, 28998), (29474, 29479), (33427, 33432), (34894, 34899), (34905, 34910), (69902, 69907), (76637, 76642), (81060, 81065), (88639, 88644), (106108, 106113), (111491, 111496), (113127, 113132), (115740, 115745), (116146, 116151), (119841, 119846), (135605, 135610), (136136, 136141), (136495, 136500), (137107, 137112), (141280, 141285), (146234, 146239), (150460, 150465), (164675, 164680), (166841, 166846), (170651, 170656), (177143, 177148), (189222, 189227), (198956, 198961), (218348, 218353), (225894, 225899), (235596, 235601), (237125, 237130), (260375, 260380), (263518, 263523), (290355, 290360), (291127, 291132), (291999, 292004), (297965, 297970), (299531, 299536), (299651, 299656), (304082, 304087), (323664, 323669), (347268, 347273), (347287, 347292), (348333, 348338), (352391, 352396), (355282, 355287), (357044, 357049), (365364, 365369), (368979, 368984), (379925, 379930), (381894, 381899), (425952, 425957), (451826, 451831), (453630, 453635), (453932, 453937), (459281, 459286), (459400, 459405), (461078, 461083), (461995, 462000), (470683, 470688), (470810, 470815), (482149, 482154), (499526, 499531), (508410, 508415), (545322, 545327), (568696, 568701), (569114, 569119), (570261, 570266), (570763, 570768), (571064, 571069), (571602, 571607), (573523, 573528), (573559, 573564), (573591, 573596), (574517, 574522), (582985, 582990), (592646, 592651), (593134, 593139), (595313, 595318), (595386, 595391), (603358, 603363), (612371, 612376), (621568, 621573), (623283, 623288), (623770, 623775), (625404, 625409), (627297, 627302), (629500, 629505), (642350, 642355), (656870, 656875), (659686, 659691), (660173, 660178), (660478, 660483), (661048, 661053), (661763, 661768), (662454, 662459), (663615, 663620), (665450, 665455), (669256, 669261), (669765, 669770), (672951, 672956), (675403, 675408), (675808, 675813), (677698, 677703), (679069, 679074), (685329, 685334), (690790, 690795), (690975, 690980), (692641, 692646), (693562, 693567), (693844, 693849), (694807, 694812), (695453, 695458), (698950, 698955), (700720, 700725), (700744, 700749), (702210, 702215), (702384, 702389), (707226, 707231), (708047, 708052), (722352, 722357), (742502, 742507), (750434, 750439), (767931, 767936), (777141, 777146), (780983, 780988), (780998, 781003), (781311, 781316), (781615, 781620), (781826, 781831), (786872, 786877), (787012, 787017), (787352, 787357), (788886, 788891), (789366, 789371), (803383, 803388), (804013, 804018), (809880, 809885), (813390, 813395), (817350, 817355), (847405, 847410), (848699, 848704), (850043, 850048), (850106, 850111), (855941, 855946), (856068, 856073), (867205, 867210)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x1c452695e20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # notez qu'importer à nouveau la librairie n'est pas nécessaire\n",
    "pattern = re.compile(\"spice\", re.IGNORECASE)\n",
    "\n",
    "# cherche la première occurrence seulement\n",
    "# res = pattern.search(texte)  \n",
    "\n",
    "# cherche toutes les occurrences\n",
    "res = pattern.finditer(texte)\n",
    "\n",
    "#start_pattern = [m.start() for m in res]\n",
    "\n",
    "#print(len(start_pattern))\n",
    "sp = [m.span() for m in res]\n",
    "print(sp)\n",
    "\n",
    "#print(texte[start_pattern[100]-10 : start_pattern[100]+20])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on souhaite trouver toutes les occurrences, on peut afficher ce qu'on appelle un concordancier.\n",
    "\n",
    "Depuis le XIIIème siècle, un **concordancier** est une liste triée alphabétiquement des principaux mots employés dans un corpus, précisant **chaque instance** des mots accompagnée de leur **contexte immédiat**. \n",
    "\n",
    "<img src=\"img/concordance.jpg\" style='height: 400px'/>\n",
    "\n",
    "*Cruden's Concordance (concordance of the King James Bible that was single-handedly created by Alexander Cruden)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "window = 50\n",
    "\n",
    "def concord(texte, pat):\n",
    "    pattern = re.compile(pat)\n",
    "    res = pattern.finditer(texte)\n",
    "    pos_pattern = [m.span() for m in res]\n",
    "    context_left = pd.DataFrame([texte[i-window:i-1] for (i, j) in pos_pattern])\n",
    "    center = pd.DataFrame([texte[i: j] for (i, j) in pos_pattern])\n",
    "    context_right = pd.DataFrame([texte[j+1:j+window] for (i, j) in pos_pattern])\n",
    "    return (pd.concat([context_left, center, context_right], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques exemples d'expressions régulières :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>round the Lady Jessica, reaching far out into the</td>\n",
       "      <td>dun</td>\n",
       "      <td>flatness of the landing plain upon which her tran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arachans -- those high, crescent-shaped migratory</td>\n",
       "      <td>dune</td>\n",
       "      <td>which moved like waves around Arrakis. This was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rrakis. This was Kedem, the inner desert, and its</td>\n",
       "      <td>dune</td>\n",
       "      <td>were rarely marked these days by the irregularit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m's progress. Sunset drew bloody streaks over the</td>\n",
       "      <td>dune</td>\n",
       "      <td>, imparting a fiery light to the shadow edges. A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and green.     A diamond-shaped oasis of planted</td>\n",
       "      <td>dune</td>\n",
       "      <td>spread beneath his high perch, focusing his atte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>fficult to take his gaze away from the sands, the</td>\n",
       "      <td>dune</td>\n",
       "      <td>-- the great emptiness. Here at the edge of the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>e dust, the sparse and lonely plants and animals,</td>\n",
       "      <td>dune</td>\n",
       "      <td>merging into dune, desert into desert.     Behind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>and lonely plants and animals, dune merging into</td>\n",
       "      <td>dune</td>\n",
       "      <td>desert into desert.     Behind him came the soun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>him. Now Stilgar and Irulan waited in the Temple</td>\n",
       "      <td>dun</td>\n",
       "      <td>eons. Perhaps they would die, but there might be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>No wind ever ran as he runs. He's a blur atop the</td>\n",
       "      <td>dune</td>\n",
       "      <td>. I've seen him. He runs and runs. And when he ha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0     0  \\\n",
       "0   round the Lady Jessica, reaching far out into the   dun   \n",
       "1   arachans -- those high, crescent-shaped migratory  dune   \n",
       "2   rrakis. This was Kedem, the inner desert, and its  dune   \n",
       "3   m's progress. Sunset drew bloody streaks over the  dune   \n",
       "4    and green.     A diamond-shaped oasis of planted  dune   \n",
       "..                                                ...   ...   \n",
       "75  fficult to take his gaze away from the sands, the  dune   \n",
       "76  e dust, the sparse and lonely plants and animals,  dune   \n",
       "77   and lonely plants and animals, dune merging into  dune   \n",
       "78   him. Now Stilgar and Irulan waited in the Temple   dun   \n",
       "79  No wind ever ran as he runs. He's a blur atop the  dune   \n",
       "\n",
       "                                                    0  \n",
       "0   flatness of the landing plain upon which her tran  \n",
       "1    which moved like waves around Arrakis. This was   \n",
       "2    were rarely marked these days by the irregularit  \n",
       "3   , imparting a fiery light to the shadow edges. A   \n",
       "4    spread beneath his high perch, focusing his atte  \n",
       "..                                                ...  \n",
       "75   -- the great emptiness. Here at the edge of the   \n",
       "76  merging into dune, desert into desert.     Behind  \n",
       "77   desert into desert.     Behind him came the soun  \n",
       "78  eons. Perhaps they would die, but there might be   \n",
       "79  . I've seen him. He runs and runs. And when he ha  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suite exacte :\n",
    "#concord(texte, \"dune\")\n",
    "\n",
    "# présence optionnelle d'un caractère :\n",
    "concord(texte, \"dune?\")\n",
    "\n",
    "# chiffre :\n",
    "#concord(texte, \"[0-9]\")\n",
    "\n",
    "# suite de chiffres :\n",
    "#concord(texte, \"[0-9]+\")   # le + indique qu'il doit au moins y avoir un chiffre\n",
    "#concord(texte, \"\\d+\")   # \\d est plus général pour les suites de chiffre\n",
    "\n",
    "# recherche de deux motifs :\n",
    "#concord(texte, \"Paul|Atreid\")    # le | (ou \"pipe\") indique un OU logique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'action la plus élémentaire consiste à nettoyer le texte s'il contient des symboles non souhaités (par ex. des caractères unicodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Voilà un texte qui pose problème - voyons ce que l'on peut y faire\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "txt_a_nettoyer = \"ÅVoilà un texte qui pose problͰme  ̶ voyons ce que l ̕on peut y faire\"\n",
    "\n",
    "txt_propre = re.sub(\"Å\", \" \", txt_a_nettoyer)\n",
    "txt_propre = re.sub(\"Ͱ\", \"è\", txt_propre)\n",
    "txt_propre = re.sub(\" ̶\", \"-\", txt_propre)\n",
    "txt_propre = re.sub(\" ̕\", \"'\", txt_propre)\n",
    "\n",
    "print(txt_propre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Voilà un texte qui pose problème - voyons ce que l'on peut y faire\n"
     ]
    }
   ],
   "source": [
    "clean_unicode = {\n",
    "  \"Å\": \" \",\n",
    "  \"Ͱ\": \"è\",\n",
    "  \" ̶\" : \"-\",\n",
    "  \" ̕\" : \"'\"\n",
    "}\n",
    "\n",
    "txt_propre_2 = txt_a_nettoyer\n",
    "for c in clean_unicode:\n",
    "    txt_propre_2 = re.sub(c, clean_unicode[c], txt_propre_2)\n",
    "    \n",
    "print(txt_propre_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également simple de remplacer une expression par une autre, par exemple les occurrences de \"chaumas\" par \"poison\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la fonction re.sub permet de faire un \"remplacer tout\" pour les occurrences d'un motif donné\n",
    "# attention à penser à sauvegarder le résultats, c'est-à-dire la nouvelle chaîne de caractères\n",
    "\n",
    "texte_comp = re.sub(\"chaumas\", \"poison\", texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ill it be chaumas--poison in the food?     He shook his head\n",
      "a drop of poison on its tip. Ah-ah! Don't pull away or you'l\n"
     ]
    }
   ],
   "source": [
    "# on vérifie que le remplacement a bien été réalisé\n",
    "\n",
    "num_it = 0 # numéro de l'instance qu'on souhaite vérifier\n",
    "\n",
    "liste_it_texteini = [m.start() for m in re.finditer(\"chaumas\", texte)]\n",
    "print(texte[liste_it_texteini[num_it]-10: liste_it_texteini[num_it]+50])\n",
    "\n",
    "liste_it_textetransf = [m.start() for m in re.finditer(\"poison\", texte_comp)]\n",
    "print(texte_comp[liste_it_textetransf[num_it]-10: liste_it_textetransf[num_it]+50])\n",
    "#print(sp_pgpp_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser des expressions régulières plus complexes.\n",
    "\n",
    "Par exemple pour chercher tous les nombres qui peuvent comporter des espaces, des virgules ou des points, dans leur écriture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1965</td>\n",
       "      <td>ook 1 DUNE  = = = = = =   A beginning is the time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>UNE  = = = = = =   A beginning is the time for ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that you first place him in his time: born in the</td>\n",
       "      <td>57</td>\n",
       "      <td>h year of the Padishah Emperor, Shaddam IV. And t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= =  YUEH (yu'e), Wellington (weling-tun), Stdrd</td>\n",
       "      <td>10,082</td>\n",
       "      <td>10,191; medical doctor of the Suk School (grd Std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UEH (yu'e), Wellington (weling-tun), Stdrd 10,082</td>\n",
       "      <td>10,191</td>\n",
       "      <td>medical doctor of the Suk School (grd Stdrd 10,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>rice on the Imperial market has ranged as high as</td>\n",
       "      <td>620,000</td>\n",
       "      <td>olaris the decagram. MENTAT: that class of Imperi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>quartz. Noted for extreme tensile strength (about</td>\n",
       "      <td>450,000</td>\n",
       "      <td>ilos per square centimeter at two centimeters' th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>orms grow to enormous size (specimens longer than</td>\n",
       "      <td>400</td>\n",
       "      <td>eters have been seen in the deep desert) and live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>e-dimensional image from a solido projector using</td>\n",
       "      <td>360</td>\n",
       "      <td>degree reference signals imprinted on a shigawire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>of Maometh (the so-called \"Third Muhammed\") about</td>\n",
       "      <td>1381</td>\n",
       "      <td>.G. The Zensunni religion is noted chiefly for it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0         0  \\\n",
       "0                                                        1965     \n",
       "1                                                            1    \n",
       "2   that you first place him in his time: born in the        57   \n",
       "3    = =  YUEH (yu'e), Wellington (weling-tun), Stdrd    10,082   \n",
       "4   UEH (yu'e), Wellington (weling-tun), Stdrd 10,082    10,191   \n",
       "..                                                ...       ...   \n",
       "81  rice on the Imperial market has ranged as high as  620,000    \n",
       "82  quartz. Noted for extreme tensile strength (about  450,000    \n",
       "83  orms grow to enormous size (specimens longer than      400    \n",
       "84  e-dimensional image from a solido projector using       360   \n",
       "85  of Maometh (the so-called \"Third Muhammed\") about     1381    \n",
       "\n",
       "                                                    0  \n",
       "0   ook 1 DUNE  = = = = = =   A beginning is the time  \n",
       "1   UNE  = = = = = =   A beginning is the time for ta  \n",
       "2   h year of the Padishah Emperor, Shaddam IV. And t  \n",
       "3   10,191; medical doctor of the Suk School (grd Std  \n",
       "4    medical doctor of the Suk School (grd Stdrd 10,1  \n",
       "..                                                ...  \n",
       "81  olaris the decagram. MENTAT: that class of Imperi  \n",
       "82  ilos per square centimeter at two centimeters' th  \n",
       "83  eters have been seen in the deep desert) and live  \n",
       "84  degree reference signals imprinted on a shigawire  \n",
       "85  .G. The Zensunni religion is noted chiefly for it  \n",
       "\n",
       "[86 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_pat = \"[0-9]+((\\s|,|\\.)|[0-9])+\"   #\\s indique n'importe quel espace\n",
    "\n",
    "concord(texte, int_pat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un nettoyage standard pour les analyses ultérieures consiste à passer tout le corpus en minuscule. C'est une forme de normalisation permettant de rapprocher des termes comme \"Unité\" et \"unité\" par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avant :  e glittering jewels.     \"Is he not small for his age, Jessica?\" the old woman asked. Her voice whee\n",
      "après :  e glittering jewels.     \"is he not small for his age, jessica?\" the old woman asked. her voice whee\n"
     ]
    }
   ],
   "source": [
    "print(\"avant : \", texte[1500:1600])\n",
    "print(\"après : \", texte[1500:1600].lower())\n",
    "\n",
    "texte_minuscules = texte.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques prétraitements à connaître\n",
    "\n",
    "- expressions réguières et nettoyages simples\n",
    "- **segmentation en mots (*tokenization*)**\n",
    "- mots-outils\n",
    "- stemming et lemmatisation\n",
    "- n-grammes et collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segmentation en mots (*tokenization*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split a string into basic (lexical) **units**, such as words.\n",
    "\n",
    "Different rules for different languages:\n",
    "\n",
    "我已经等我的包裹三星期了！\n",
    "<br/>\n",
    "(I've been waiting three weeks for my parcel to arrive!)\n",
    "\n",
    "En français,c'est assez simple mais<br/>ilfaut faire attention<br/>notamment sur plusieurs lignes.\n",
    "\n",
    "Even in English or French, you can have difficulties splitting strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is for sure a small paragraph Id like to parse\\non multiple linesIs this that simpleI guess not'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chaine = \"Here is, for sure, a small paragraph I'd like to parse\\non multiple lines.Is this that simple?I guess not.\"\n",
    "re.sub(\"[^(\\s|\\w)]\", \"\", chaine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En général, avec la plupart des langues européennes, c'est une tâche assez facile si on utilise les séparateurs suivants :\n",
    "\n",
    ", . \\n \\t - : ; ( ) ! ? [ ] _ ' \"  (etc.)\n",
    "\n",
    "Mais... :\n",
    "\n",
    ">\"Harry Potter\" => \"Harry\", \"Potter\"<br/>\n",
    ">\"rez-de-chaussée\" => \"chaussée\", \"de\", \"rez\"<br/>\n",
    ">\"idiot?\" => \"idiot\"<br/>\n",
    ">\"C.E.O\" => \"C\", \"E\", \"O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons voir dans les carnets qui suivant et que les librairies Python comme *nltk*, *scikit-learn* ou *spacy* embarquent des tokeniseurs puissants.\n",
    "\n",
    "Cependant, essayons de réaliser cette opération manuellement afin de mesurer les mécanismes sous-jacents avec le texte suivant :\n",
    "\n",
    "\"The challenge of exploiting the large proportion of enterprise information that originates in \"unstructured\" form has been recognized for decades.[7] It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:\"<br/>(excerpt of https://en.wikipedia.org/wiki/Text_mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'challenge',\n",
       " 'of',\n",
       " 'exploiting',\n",
       " 'the',\n",
       " 'large',\n",
       " 'proportion',\n",
       " 'of',\n",
       " 'enterprise',\n",
       " 'information',\n",
       " 'that',\n",
       " 'originates',\n",
       " 'in',\n",
       " 'unstructured',\n",
       " 'form',\n",
       " 'has',\n",
       " 'been',\n",
       " 'recognized',\n",
       " 'for',\n",
       " 'decades',\n",
       " '7',\n",
       " 'It',\n",
       " 'is',\n",
       " 'recognized',\n",
       " 'in',\n",
       " 'the',\n",
       " 'earliest',\n",
       " 'definition',\n",
       " 'of',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " 'BI',\n",
       " 'in',\n",
       " 'an',\n",
       " 'October',\n",
       " '1958',\n",
       " 'IBM',\n",
       " 'Journal',\n",
       " 'article',\n",
       " 'by',\n",
       " 'H',\n",
       " 'P',\n",
       " 'Luhn',\n",
       " 'A',\n",
       " 'Business',\n",
       " 'Intelligence',\n",
       " 'System',\n",
       " 'which',\n",
       " 'describes',\n",
       " 'a',\n",
       " 'system',\n",
       " 'that',\n",
       " 'will',\n",
       " '']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch = \"The challenge of exploiting the large proportion of enterprise information that originates in \\\"unstructured\\\" form has been recognized for decades.[7] It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:\"\n",
    "\n",
    "re.split(r'\\W+', ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques prétraitements à connaître\n",
    "\n",
    "- expressions réguières et nettoyages simples\n",
    "- segmentation en mots (*tokenization*)\n",
    "- **mots-outils**\n",
    "- stemming et lemmatisation\n",
    "- n-grammes et collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mots-outils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Commençons par quelques observations générales sur des corpus bien connus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Mots les plus fréquents observés dans Harry Potter :\n",
    "\n",
    "<img src=\"img/zipf_1.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette forme particulière est appelée \"loi de Zipf\". Elle a été observée pour la première fois par le linguiste George K. Zipf (1902-1950).\n",
    "\n",
    "La loi de Zipf indique que si l'on observe un corpus suffisamment grand, la fréquence d'apparition d'un mot est **inversement proportionnelle** à son rang dans la table des fréquences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mots-outils sont des mots qui n'apportent pas beaucoup d'information et sont surtout utilisés pour construire les phrases. Ils jouent donc un rôle *fonctionnel*.\n",
    "\n",
    "Il faut faire un peu attention mais, la plupart du temps, on les enlève."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie *scikit-learn* fournit ses propres listes de mots-outils, mais *nltk* a une bibliothèque plus fournie de langues. Il est bien entendu possible de charger votre propre liste de mots-outils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# il ne faut pas oublier de télécharger les ressources nécessaires, en particulier le module \"stopwords\" de \"Corpora\"\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots-outils en anglais : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n",
      "Mots-outils en français : ['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur']\n",
      "Mots-outils en arabe : ['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(\"Mots-outils en anglais : {}\".format(stopwords.words('english')[0:20]))\n",
    "print(\"Mots-outils en français : {}\".format(stopwords.words('french')[0:20]))\n",
    "print(\"Mots-outils en arabe : {}\".format(stopwords.words('arabic')[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut tester ces listes à la main en passant par un *tokenizer* maison sous nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'simple', 'example', 'sentence', 'multiple', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "\n",
    "example_sent = \"Here is a simple example of a sentence with multiple stopwords\"\n",
    "  \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "word_tokens = word_tokenize(example_sent)  \n",
    "word_tokens\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suivant l'hypothèse du \"petit monde\", on peut développer un algorithme simple mais efficace d'identification de la langue si la longueur d'un texte est assez grande.\n",
    "\n",
    "Si $|T| > 30$, alors on obtient 99,5% de réussite (Grefenstette 1995)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 1, 0, 1, 0, 2, 3, 1, 2, 4, 0, 6, 1]\n",
      "My guess is: spanish\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "languages = (\n",
    "        \"arabic\",\n",
    "        \"danish\",\n",
    "        \"dutch\",\n",
    "        \"english\",\n",
    "        \"finnish\",\n",
    "        \"french\",\n",
    "        \"german\",\n",
    "        \"hungarian\",\n",
    "        \"italian\",\n",
    "        \"norwegian\",\n",
    "        \"portuguese\",\n",
    "        \"romanian\",\n",
    "        \"russian\",\n",
    "        \"spanish\",\n",
    "        \"swedish\",\n",
    "    )\n",
    "\n",
    "ch = \"Las Antillas, o islas del Caribe, estan situadas junto al tropico de Cancer. Alli hace calor todo el ano.\"\n",
    "#ch = \"Voilà un test en français un peu plus long et on continue\"\n",
    "#ch = \"hello there, how are you guys?\"\n",
    "\n",
    "# compte le nombre de mots-outils pour une langue donnée\n",
    "def nb_stopwords(s, langue):\n",
    "    stop_words = set(stopwords.words(langue))\n",
    "    word_tokens = set(word_tokenize(s))\n",
    "    return len(stop_words.intersection(word_tokens))\n",
    "\n",
    "list_val = [nb_stopwords(ch, l) for l in languages]\n",
    "\n",
    "print(list_val)\n",
    "print(\"My guess is: {}\".format(languages[np.argmax(list_val)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Même graphique mais en supprimant les mots-outils anglais :\n",
    "    \n",
    "<img src=\"img/zipf_2.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques prétraitements à connaître\n",
    "\n",
    "- expressions réguières et nettoyages simples\n",
    "- segmentation en mots (*tokenization*)\n",
    "- mots-outils\n",
    "- **stemming et lemmatisation**\n",
    "- n-grammes et collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## racinisation et lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "D'autres prétraitements sont également disponibles, tels que :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* racinisation (*stemming*) : trouver la racine des mots, comme dans :\n",
    "     \n",
    "> learn: learns, learned, learning...<br/>\n",
    "> march: marcher, marchera, marcherai...\n",
    " \n",
    "* lemmatisation (*lemmatization*) : trouver le lemme, comme dans :\n",
    "\n",
    "> to be: am, are<br/>\n",
    "> être : suis, sont...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "La racinisation (*stemming*) nous aide à :\n",
    "\n",
    "- rapprocher deux documents sémantiquement liés mais n'utilisant pas exactement les mêmes termes\n",
    "- réduire la taille du vocabulaire (cf. malédiction de la dimension)\n",
    "\n",
    "Ok pour :\n",
    "\n",
    "    adventur: [adventure,adventurer,adventurers,adventures,adventurous]\n",
    "\n",
    "mais...\n",
    "\n",
    "    anim: [animal, animals, animation]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer_fr = SnowballStemmer(\"french\")\n",
    "stemmer_en = SnowballStemmer(\"english\") # à priori, l'algorithme de Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En français :\n",
      "['march', 'march', 'march', 'march']\n",
      "['aventur', 'aventur', 'aventuri', 'aventur']\n",
      "En anglais :\n",
      "['adventur', 'adventur', 'adventur', 'adventur', 'adventur']\n"
     ]
    }
   ],
   "source": [
    "#stemmer.stem(\"maisons\")\n",
    "print(\"En français :\")\n",
    "print([stemmer_fr.stem(w) for w in [\"marcher\", \"marcherons\", \"marcherait\", \"marché\"]])\n",
    "print([stemmer_fr.stem(w) for w in [\"aventure\", \"aventures\", \"aventuriers\", \"aventureux\"]])\n",
    "print(\"En anglais :\")\n",
    "print([stemmer_en.stem(w) for w in [\"adventures\", \"adventurers\", \"adventurous\", \"adventurer\", \"adventurous\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'here': 2, 'is': 3, 'the': 5, 'adventur': 0, 'of': 4, 'an': 1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour l'utilisation de CountVectorizer avec la librairie scikit-learn, voir le carnet 2.1 :\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words_en(doc):\n",
    "    return (stemmer_en.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer_en = CountVectorizer(analyzer=stemmed_words_en)\n",
    "\n",
    "X = [\"Here is the adventures of an adventurous adventurer\"]\n",
    "stem_vectorizer_en.fit(X)\n",
    "stem_vectorizer_en.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lemmatisation (*lemmatization*) transforme les mots en leur lexème sous-jacent (*lemma*).\n",
    "\n",
    "> running --> to run (verb) / running (noun)<br/>\n",
    "> are &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--> to be (verb)\n",
    "\n",
    "Pour cela, il est nécessaire de résoudre le problème de détection des catégories grammaticales (*POS tagging*) et de recourir à une base de connaissances lexicales, voire des données annotées.\n",
    "\n",
    "La lemmatisation est plus précise que la racinisation, mais elle nécessite plus de ressources. Cela revient à un choix entre + de précision et - de rappel (lemmatisation) vs. - de précision et + de rappel (racinisation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant nltk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "sentence = \"Mark and John are working at Google.\"\n",
    " \n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais également la library stanza (https://stanfordnlp.github.io/stanza/) qui fonctionne en général bien dans beaucoup de langues :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 09:39:40 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2021-12-17 09:39:40 INFO: Use device: cpu\n",
      "2021-12-17 09:39:40 INFO: Loading: tokenize\n",
      "2021-12-17 09:39:40 INFO: Loading: mwt\n",
      "2021-12-17 09:39:40 INFO: Loading: pos\n",
      "2021-12-17 09:39:40 INFO: Loading: lemma\n",
      "2021-12-17 09:39:40 INFO: Loading: depparse\n",
      "2021-12-17 09:39:40 INFO: Loading: ner\n",
      "2021-12-17 09:39:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "#stanza.download('fr') # modèle pour le français\n",
    "#   (attention, c'est assez lourd ~500Mo, il vaut mieux avoir une bonne connexion)\n",
    "nlp = stanza.Pipeline('fr') # pipeline pour le français\n",
    "doc = nlp(\"Barack Obama est né à Hawaii.\") # premier test d'annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"Barack\",\n",
       "      \"lemma\": \"Barack\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 6,\n",
       "      \"ner\": \"B-PER\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"Obama\",\n",
       "      \"lemma\": \"Obama\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"head\": 1,\n",
       "      \"deprel\": \"flat:name\",\n",
       "      \"start_char\": 7,\n",
       "      \"end_char\": 12,\n",
       "      \"ner\": \"E-PER\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"est\",\n",
       "      \"lemma\": \"être\",\n",
       "      \"upos\": \"AUX\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"aux:tense\",\n",
       "      \"start_char\": 13,\n",
       "      \"end_char\": 16,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"né\",\n",
       "      \"lemma\": \"naître\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"feats\": \"Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 17,\n",
       "      \"end_char\": 19,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"à\",\n",
       "      \"lemma\": \"à\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 20,\n",
       "      \"end_char\": 21,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"Hawaii\",\n",
       "      \"lemma\": \"Hawaii\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"obl:arg\",\n",
       "      \"start_char\": 22,\n",
       "      \"end_char\": 28,\n",
       "      \"ner\": \"S-LOC\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 28,\n",
       "      \"end_char\": 29,\n",
       "      \"ner\": \"O\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réécriture du texte avec les lemmes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack', 'Obama', 'être', 'naître', 'à', 'Hawaii', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.lemma for w in doc.sentences[0].words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aller plus loin en identifiant les entités nommées, par exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "texte_rois = \"\"\n",
    "with open(os.path.join(\"datasets\", \"rois.txt\"), \"r\", encoding='utf8') as f:\n",
    "    texte_rois = texte_rois.join(line.rstrip(\"\\n\") + \" \" for line in f.readlines())\n",
    "\n",
    "# la ligne suivante permet de retirer les espaces redondantes à l'intérieur de la chaîne de caractères\n",
    "#texte_rois = \" \".join(texte_rois.split())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_rois = nlp(texte_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAURICE DRUON PER\n",
      "Académie française ORG\n",
      "ROIS MAUDITS MISC\n",
      "LIVRE I  Le Roi de Fer MISC\n",
      "Le Roi de fer MISC\n",
      "Maurice Druon PER\n",
      "Pion PER\n",
      "Éditions Del Duca MISC\n",
      "Edmond PER\n",
      "Jules de Concourt      PROLOGUE PER\n",
      "Philippe IV PER\n",
      "la France LOC\n",
      "Flamands MISC\n",
      "Anglais MISC\n",
      "Aquitaine LOC\n",
      "Papauté MISC\n",
      "Avignon LOC\n",
      "Parlements MISC\n",
      "Edouard II d’Angleterre PER\n",
      "Russie LOC\n",
      "Aucune LOC\n",
      "Église, spolié les Juifs MISC\n",
      "Trésor ORG\n",
      "Les LOC\n",
      "Les LOC\n",
      "Les LOC\n",
      "État LOC\n",
      "la France LOC\n",
      "Français MISC\n",
      "Ordre souverain des chevaliers du Temple ORG\n",
      "Cette LOC\n",
      "Templiers MISC\n",
      "Philippe le Bel PER\n",
      "Histoire MISC\n",
      "Toutes LOC\n",
      "PREMIÈRE PARTIE ORG\n",
      "MALÉDICTION      I    LA REINE PER\n",
      "AMOUR MISC\n",
      "Angleterre LOC\n",
      "Isabelle PER\n",
      "Guillaume d’Aquitaine PER\n",
      "Ah ! MISC\n",
      "Guillaume PER\n",
      "Madame PER\n",
      "Non PER\n",
      "Monseigneur d’Artois PER\n",
      "Veillez MISC\n",
      "France LOC\n",
      "Madame PER\n",
      "Jeanne de Joinville PER\n",
      "Roger Mortimer PER\n",
      "Angleterre LOC\n"
     ]
    }
   ],
   "source": [
    "#Find named entities, phrases and concepts\n",
    "c = 0\n",
    "for entity in doc_rois.ents:\n",
    "    print(entity.text, entity.type)\n",
    "    if c>50:\n",
    "        break;\n",
    "    c +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['la France',\n",
       " 'Aquitaine',\n",
       " 'Avignon',\n",
       " 'Russie',\n",
       " 'Aucune',\n",
       " 'Les',\n",
       " 'Les',\n",
       " 'Les',\n",
       " 'État',\n",
       " 'la France',\n",
       " 'Cette',\n",
       " 'Toutes',\n",
       " 'Angleterre',\n",
       " 'France',\n",
       " 'Angleterre',\n",
       " 'France',\n",
       " 'Angleterre',\n",
       " 'France',\n",
       " 'Artois',\n",
       " 'Artois',\n",
       " 'Artois',\n",
       " 'Angleterre',\n",
       " 'Artois',\n",
       " 'Navarre',\n",
       " 'France',\n",
       " 'France',\n",
       " 'Artois',\n",
       " 'France',\n",
       " 'Or',\n",
       " 'Damas',\n",
       " 'Artois',\n",
       " 'Conches',\n",
       " 'hôtel de Nesle',\n",
       " 'Rouées',\n",
       " 'tour de Nesle',\n",
       " 'Tour',\n",
       " 'comté d’Artois',\n",
       " 'Bourgogne',\n",
       " 'Artois',\n",
       " 'Artois',\n",
       " 'Bourgogne',\n",
       " 'Allemagne',\n",
       " 'château de Conches',\n",
       " 'Beaumont',\n",
       " 'France',\n",
       " 'Bourgogne',\n",
       " 'Artois',\n",
       " 'Artois',\n",
       " 'Paris',\n",
       " 'Artois']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_loc = [ent.text for ent in doc_rois.entities if ent.type==\"LOC\"]\n",
    "list_loc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Majorque', 'Brienne', 'Beauvais', 'Aucune', 'comté de Beaumont-le-Roger', 'Brésil', 'Neauphle', 'Bordeaux', 'Franche-Comté', 'Piacenza', 'comtes d’Artois', 'Sceaux', 'Cistercien', 'Vénétie', 'Forez', 'Jérusalem', 'Escot', 'Hongrie', 'Presles', 'ROUTE DE CLERMONT', 'Trésor', 'Oise', 'l’Espagne', 'Béthune', 'rue Saint-Merri', 'Boulogne', 'Hôtel', 'Middlesex', 'Montacute', 'Nogent-le-Roi', 'Car', 'Broderies', 'York', 'Petit-Pré-aux-Clercs', 'Mons-en-Pévèle', 'Rome', 'Chirk', 'Seigneur de Fontenay', 'Saint-Siège', 'Montaigu', 'Neauphle-le-Vieux', 'Milan', 'Évreux', 'Grand Nesle', 'Clermont', 'Arbois', 'Cornouailles', 'Mirepoix', 'ÉTAIT', 'Fréjus', 'VII', 'Vémars', 'comté de Flandre', 'Windsor', 'Achaïe', 'Notre', 'Beaune', 'Vélines', 'Toutes', 'hôtel d’Artois', 'Afrique', 'Grèce', 'rue aux Sorcières', 'Santa Maria dei Servi', 'Pensez', 'Petit Nesle', 'Montfaucon', 'Galerie mercière', 'Roncheville', 'Bigorre', 'Lorraine', 'Alips de Mons', 'archidiocèse de Sens', 'Tombeau du Christ', 'Navarraise', 'Gournay', 'Lombard Street', 'Issoudun', 'Chartres', 'Quand', 'Alençon', 'Mieux', 'fresques de l’Incoronata', 'Picardie', 'Bar', 'DEUXIÈME', 'Namur', 'Hertfordshire', 'Courtille', 'La Forêt', 'Santa Maria della Scala', 'ROUTE', 'Langres', 'Ile-de-France', 'Boisroger', 'Montdidier', 'Malines', 'comté de Guines', 'Rhône', 'Courcelles-la-Garenne', 'Evreux', 'Couvin', 'Calais', 'Bourgogne-Duché', 'Luxembourg', 'Galles', 'Venise', 'Coutances', 'Roquemaure', 'Saint-Michel', 'Senlis', 'Mais', 'Arras', 'Évêque d’Albano', 'Oxford', 'Gênes', 'Conches', 'Saint-Merry', 'hôtel du Temple', 'Bucy', 'Spolète', 'Cité', 'Albi', 'Limoges', 'Nesle', 'Moyen-Orient', 'Normandie', 'Tamise', 'comte de Hainaut', 'rue Mauconseil', 'Saintonge', 'Portefruit', 'Italie', 'Vénitiens', 'Constantinople', 'TOUR', 'Berkeley', 'cloître Saint-Merry', 'Porto', 'Clichy', 'Tunis', 'Essonne', 'Licques', 'Westminster', 'Saint-Félix de Caraman', 'Brie', 'Foix', 'Trésor royal', 'château de Gournay', 'Reims', 'palais Caëtani', 'palais Tolomei', 'palais de la Cité', 'Asnières', 'Langley', 'Institut de France et de la Monnaie', 'Briançon', 'Pays de Galles', 'Saint-Martin', 'Lormet', 'comte de Kent', 'Saint-Front de Périgueux', 'Lombard', 'Suisse', 'place de Grève', 'château de Maubuisson', 'église des Jacobins', 'Parloir aux Bourgeois', 'Paris[5', 'Légumes', 'Savoie', 'Table des Matières  PROLOGUE  PREMIÈRE PARTIE LA MALÉDICTION  I LA REINE SANS AMOUR    II LES PRISONNIERS DU TEMPLE    III LES BRUS DU ROI    IV NOTRE', 'Carpentras', 'Montpellier', 'Arabie', 'Monte-Falcone', 'Saint-Paul de Londres', 'Chancelade', 'Beaumont', 'Cette', 'Espagne', 'comte de Foix', 'forêt de Pont-Sainte-Maxence', 'Normandie Roto Impression S.A.  61250 Lonrai', 'Egypte pharaonique', 'Bourgeois', 'Pampelune', 'Arrière', 'Conflans', 'Toscans', 'Tour de Londres', 'Aunay', 'rue Mazarine', 'Aragon', 'Blois', 'Leicester', 'Dourdan', 'Maine', 'Latium', 'Saint-Germain-des-Prés', 'Florence', 'Saint-Jean-d’Acre', 'Sarlat', 'Manche', 'Siennois', 'Anseau', 'Auch', 'Bouville', 'Europe', 'Notre-Dame de Boulogne', 'Orléans', 'Courtrai', 'Angoulême', 'Campanie', 'Douai', 'Le Portier', 'hôtel Tolomei', 'Champagne', 'France', 'Jalemain', 'Provence', 'rue des Bourdonnais', 'Auxerre', 'jardin du Palais', 'Gardien', 'Diable', 'Boulogne-sur-Mer', 'Rogations', 'Charnay', 'Poissy', 'Chypre', 'Notre-Dame', 'SUR LE ROYAUME', 'Temple', 'Un', 'Poitou', 'tour de l’Eau', 'Saint-Denis', 'Outre-mer', 'Pergame', 'Noyon', 'Douvres', 'garde des Sceaux', 'Bristol', 'Hôtel de Ville de Paris', 'Wigmore', 'Périgueux', 'Mauldre', 'comté d’Eu', 'Neauphle-le-Château', 'faubourg Saint-Marcel', 'lycée Michelet', 'comté de Valois', 'pont Saint-Michel', 'la France', 'Albano', 'Gand', 'Hirson', 'Valence', 'Porcien', 'Saint-André de Cahors', 'Saint-Hugues', 'Asie', 'Beaucaire', 'Sienne', 'diocèse de Toulouse', 'Mont-Martre', 'Maison de la marchandise', 'Ostie', 'Cour', 'Bruges', 'Hireçon', 'Toulouse', 'Bourbourg', 'Comminges', 'rue des Blancs-Manteaux', 'Châtelet', 'Cressay', 'Portugal', 'Moyen ge', 'île de la Cité', 'Le soleil', 'Anagni', 'Saint-Germain-l’Auxerrois', 'Hôtel proprement dit', 'Perche', 'Chaudes', 'Carnarvon', 'Conti', 'Périgord', 'Gironde', 'Elbeuf', 'Navarre', 'Lyonnais', 'Saint-Sauveur-le-Vicomte', 'Des', 'Sussex', 'Viennois', 'Bretagne', 'Crécy', 'Ses', 'Fontaines', 'hôtel des Templiers', 'Buonsignori', 'Castille', 'Santa Maria delle Nevi', 'Ces', 'Clermont-de-l’Oise', 'royaume de Naples', 'Dauphiné', 'Noyers', 'Fougères', 'Ponthieu', 'Monnaie de Paris', 'Sens', 'Aude', 'Zélande', 'Béarn', 'Flandres', 'Sainte-Marie Majeure', 'Dijon', 'Mainneville', 'pont Notre-Dame', 'Vaugirard', 'Les', 'Là-haut', 'Flamands', 'Tour', 'Finissons', 'Chambly', 'château de Westminster', 'Bourgogne-Comté', 'Italiens', 'Toscane', 'Rouées', 'Seine', 'château de Berkeley', 'Haute-Saône', 'Plainville-en-Vexin', 'Anjou-Sicile', 'Orient', 'Villandraut', 'Gloucester', 'Clignancourt', 'comte de Hollande', 'hôtel de Nesle', 'Surrey', 'Sans', 'Corbeil', 'Arundel', 'Anjou', 'Écossais', 'Avignon', 'royaume d’Aragon', 'comté de Bourgogne', 'Génois', 'Fournissons', 'Ardennes', 'Saint-Grégoire', 'Perse', 'Terre sainte', 'Pontoise', 'Russie', 'Gâtinais', 'Beaumont-le-Roger', 'la Seine', 'Cinque Ports', 'Orthez', 'Arles', 'Vandouvre', 'Grand-Moulin', 'Sainte-Chapelle', 'Worcester', 'Châtellerault', 'Poitiers', 'Louvre', 'Les Archives', 'Molay', 'la Croix', 'Monnaies', 'Vincennes', 'comté d’Artois', 'Joinville', 'royaume de France', 'Metz', 'Montfort-l’Amaury', 'Palais', 'Aquitaine', 'Flandre', 'Cahors', 'Watriquet Brasseniex', 'Longchamp', 'Hollande', 'Villeneuve', 'Tyburn', 'Pré-aux-Clercs', 'comte de la Marche', 'Ancien Templier', 'Bourgogne', 'Givry', 'tour du Temple', 'Marigny', 'Qui', 'Mesnil', 'Vaumain', 'Lyon', 'église des Chartreux', 'Nos', 'Arrêté', 'tour Hamelin', 'Le', 'Winchester', 'hôtel de Marigny', 'Lombard de Douvres', 'Narbonne', 'Châtillon', 'tour de Londres', 'Palestrina', 'Caumont', 'Parisiens', 'Arno', 'Occident', 'Coglione', 'Naples', 'Allemagne', 'Loos', 'Gascogne', 'Saint-Pol', 'VI', 'Château-Gaillard', 'Guigues', 'Saint-Eustache', 'PONT-SAINTE-MAXENCE', 'Vauréal', 'Amiens', 'Deux', 'Languedoc', 'Maurienne', 'Todi', 'Grez', 'Lizy-sur-Ourcq', 'comte de Flandre', 'Levant', 'archevêque de Rouen', 'Cet', 'Non', 'Remboursez', 'Comté-Franche', 'Italien', 'abbaye de Maubuisson', 'Aunay-lès-Bondy', 'Bohême', 'Bourgognes', 'La Madelaine', 'abbesse des Clarisses', 'Embrun', 'Limousin', 'Fiennes', 'Troyes', 'Le Loquetier', 'Or', 'Empire', 'Tarente', 'Béziers', 'Martroy', 'Buci', 'Arnaud', 'Pompadour', 'Châlons', 'Londoniens', 'Égidius', 'Ployecul', 'Ponte Vecchio', 'Jura', 'Écouis', 'Beaugency', 'Luzarches', 'Ne', 'Salins', 'Moucy-le-Neuf', 'Galerie', 'Perpignan', 'D’', 'château de Hertford', 'château de Conches', 'Damas', 'Parce', 'Rundingen', 'tour de Nesle', 'Norfolk', 'pont de Londres', 'Irlande', 'rue de la Bretonnerie', 'Palestine', 'pays de Liège', 'Dole', 'LONDRES', 'Grand pénitencier', 'Vannes', 'Vermandois', 'Époisses', 'TEMPLE', 'Hereford', 'Westmoutiers', 'Romagne', 'rue aux Oiseaux', 'rue aux Herbes', 'Sicile', 'Veuillez', 'Guénégaud', 'Fontfroide', 'Angevins', 'rue des Forgerons', 'Châteauneuf', 'Ceux', 'Courtenay', 'Fontainebleau', 'Saint-Maurice', 'Aix', 'Enfer', 'Londres', 'comte de Saint-Pol', 'royaume de Sicile', 'Maubuisson', 'couvent des Augustins', '(1356) la Bulle d’Or.  Charles IV', 'Artois', 'Marche', 'Araines', 'Sceaux de France', 'État', 'Nevers', 'Galerie marchande', 'Montfort', 'Paris', 'Eau', 'Auxois', 'Lyons-la-Forêt', 'Guyenne', 'Aucun', 'Longwy', 'Revel', 'Saint-Martin de Tours', 'Byzance', 'Saint-Père', 'États', 'Éclairés', 'dame Eliabel', 'Angleterre', 'tour du Louvre', 'Santa Maria del Carmine', 'Nivernais', 'Gévaudan', 'Rouen', 'Cordoue', 'Hainaut', 'Divion', 'LES', 'Pamiers', 'Palais de la Cité', 'Shrewsbury', 'Égypte ancienne', 'Notre-Dame de Paris', 'Laon', 'Blanche', 'collège Mazarin', 'Bonne', 'Venours']\n"
     ]
    }
   ],
   "source": [
    "ensemble_lieux = set(sorted(list_loc))\n",
    "print(list(ensemble_lieux))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une alternative est constituée par la librairie spacy, cf.: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques autre prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Autre prétraitement utile : remplacer certains motifs trouvés dans les textes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here is the adventures of an adventurous adventurer']\n",
      "['Here are the adventures of an adventurous adventurer']\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "X_rep = [x.replace(\"is\", \"are\") for x in X]\n",
    "print(X_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are the adventuretruc of an adventuroutruc adventurer'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[s|S]+\\w*\", \"truc\", X_rep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Corriger automatiquement l'orthographe des mots grâce à la distance d'édition (ou distance Levenshtein, proposée en 1965).\n",
    "Il s'agit de calculer le plus petit nombre d'opérations (insertion, suppression, remplacement) pour passer d'une chaîne à une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# one possible implementation from https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python\n",
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#levenshtein(\"chaîne\", \"chaine\")\n",
    "levenshtein(\"chaîne\", \"chiane\")\n",
    "#levenshtein(\"remerciement\", \"remerciments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
